{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Recommenders-4-:-Pytorch-and-Recommenders-(~1h)\" data-toc-modified-id=\"Recommenders-4-:-Pytorch-and-Recommenders-(~1h)-1\">Recommenders 4 : Pytorch and Recommenders (~1h)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objectives:\" data-toc-modified-id=\"Objectives:-1.1\">Objectives:</a></span></li></ul></li><li><span><a href=\"#(a)-WHAT-IS-PYTORCH?\" data-toc-modified-id=\"(a)-WHAT-IS-PYTORCH?-2\">(a) WHAT IS PYTORCH?</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Tensors-:-the-main-unit\" data-toc-modified-id=\"Tensors-:-the-main-unit-2.0.1\">Tensors : the main unit</a></span></li></ul></li><li><span><a href=\"#Some-useful-functions:\" data-toc-modified-id=\"Some-useful-functions:-2.1\">Some useful functions:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-tensors\" data-toc-modified-id=\"Creating-tensors-2.1.1\">Creating tensors</a></span><ul class=\"toc-item\"><li><span><a href=\"#initialize-an-empty-4x2-matrix\" data-toc-modified-id=\"initialize-an-empty-4x2-matrix-2.1.1.1\">initialize an empty 4x2 matrix</a></span></li><li><span><a href=\"#create-a-3x2-tensor-filled-with-zeros-of-type-long\" data-toc-modified-id=\"create-a-3x2-tensor-filled-with-zeros-of-type-long-2.1.1.2\">create a 3x2 tensor filled with zeros of type long</a></span></li><li><span><a href=\"#create-a-tensor-of-size-2-with-(0-=>-5.5)-and-(1-=>-3)\" data-toc-modified-id=\"create-a-tensor-of-size-2-with-(0-=>-5.5)-and-(1-=>-3)-2.1.1.3\">create a tensor of size 2 with (0 =&gt; 5.5) and (1 =&gt; 3)</a></span></li><li><span><a href=\"#create-random-5x3-and-3x5-tensors\" data-toc-modified-id=\"create-random-5x3-and-3x5-tensors-2.1.1.4\">create random 5x3 and 3x5 tensors</a></span></li><li><span><a href=\"#Indexing-works-just-like-numpy\" data-toc-modified-id=\"Indexing-works-just-like-numpy-2.1.1.5\">Indexing works just like numpy</a></span></li><li><span><a href=\"#know-the-size-of-a-tensor\" data-toc-modified-id=\"know-the-size-of-a-tensor-2.1.1.6\">know the size of a tensor</a></span></li><li><span><a href=\"#simple-addition\" data-toc-modified-id=\"simple-addition-2.1.1.7\">simple addition</a></span></li><li><span><a href=\"#matrix-multiplication\" data-toc-modified-id=\"matrix-multiplication-2.1.1.8\">matrix multiplication</a></span></li></ul></li><li><span><a href=\"#What-to-understand:\" data-toc-modified-id=\"What-to-understand:-2.1.2\">What to understand:</a></span></li><li><span><a href=\"#There-are-many-more-creation/operation-ops:\" data-toc-modified-id=\"There-are-many-more-creation/operation-ops:-2.1.3\">There are many more creation/operation ops:</a></span></li></ul></li><li><span><a href=\"#What's-interesting-beyond-the-&quot;numpy-replacement&quot;:-autodiff-!\" data-toc-modified-id=\"What's-interesting-beyond-the-&quot;numpy-replacement&quot;:-autodiff-!-2.2\">What's interesting beyond the \"numpy replacement\": autodiff !</a></span><ul class=\"toc-item\"><li><span><a href=\"#Let's-do-1d-linear-regression-with-the-vanilla-autodiff-!\" data-toc-modified-id=\"Let's-do-1d-linear-regression-with-the-vanilla-autodiff-!-2.2.1\">Let's do 1d-linear regression with the vanilla autodiff !</a></span><ul class=\"toc-item\"><li><span><a href=\"#(First)-we-need-fake-data:\" data-toc-modified-id=\"(First)-we-need-fake-data:-2.2.1.1\">(First) we need fake data:</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Full-pytorch-tutorial:\" data-toc-modified-id=\"Full-pytorch-tutorial:-3\">Full pytorch tutorial:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-used-:-smallest-movie-lens-dataset\" data-toc-modified-id=\"Data-used-:-smallest-movie-lens-dataset-3.1\">Data used : <a href=\"https://grouplens.org/datasets/movielens/\" target=\"_blank\">smallest movie-lens dataset</a></a></span></li></ul></li><li><span><a href=\"#1)--Load-&amp;-Prepare-Data\" data-toc-modified-id=\"1)--Load-&amp;-Prepare-Data-4\">1)  Load &amp; Prepare Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#(TODO)-Reproduce-the-baseline-model-with-pytorch's-vanilla-autograd\" data-toc-modified-id=\"(TODO)-Reproduce-the-baseline-model-with-pytorch's-vanilla-autograd-4.1\">(TODO) Reproduce the baseline model with pytorch's vanilla autograd</a></span></li><li><span><a href=\"#$$\\hat{r}_{ui}-=-b_{ui}-=-\\mu-+-b_u-+-b_i$$\" data-toc-modified-id=\"$$\\hat{r}_{ui}-=-b_{ui}-=-\\mu-+-b_u-+-b_i$$-4.2\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-2\">\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i</script></a></span></li><li><span><a href=\"#(TODO)-:-First,-let's-define-the-parameters\" data-toc-modified-id=\"(TODO)-:-First,-let's-define-the-parameters-4.3\">(TODO) : First, let's define the parameters</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#(TODO)-Predict-Function\" data-toc-modified-id=\"(TODO)-Predict-Function-4.3.0.1\">(TODO) Predict Function</a></span></li></ul></li><li><span><a href=\"#(TODO)-error-function\" data-toc-modified-id=\"(TODO)-error-function-4.3.1\">(TODO) error function</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-evaluation-loop,-without-any-optimization-for-now\" data-toc-modified-id=\"The-evaluation-loop,-without-any-optimization-for-now-4.3.1.1\">The evaluation loop, without any optimization for now</a></span></li></ul></li></ul></li><li><span><a href=\"#Let's-optimize-the-parameters-(with-SGD)--by-slightly-modifying-the-previous-loop\" data-toc-modified-id=\"Let's-optimize-the-parameters-(with-SGD)--by-slightly-modifying-the-previous-loop-4.4\">Let's optimize the parameters (with SGD)  by slightly modifying the previous loop</a></span><ul class=\"toc-item\"><li><span><a href=\"#(TODO)\" data-toc-modified-id=\"(TODO)-4.4.1\">(TODO)</a></span></li></ul></li></ul></li><li><span><a href=\"#Pytorch-(.nn)-Modules\" data-toc-modified-id=\"Pytorch-(.nn)-Modules-5\">Pytorch (.nn) Modules</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classic-SVD-(with-mean)\" data-toc-modified-id=\"Classic-SVD-(with-mean)-5.1\">Classic SVD (with mean)</a></span><ul class=\"toc-item\"><li><span><a href=\"#$$-\\min\\limits_{U,I}\\sum\\limits_{(u,i)}-\\underbrace{(r_{ui}----(I_i^TU_u-+-\\mu))^2}_\\text{minimization}-+-\\underbrace{\\lambda(||U_u||^2+||I_u||^2-+-\\mu)-}_\\text{regularization}-$$\" data-toc-modified-id=\"$$-\\min\\limits_{U,I}\\sum\\limits_{(u,i)}-\\underbrace{(r_{ui}----(I_i^TU_u-+-\\mu))^2}_\\text{minimization}-+-\\underbrace{\\lambda(||U_u||^2+||I_u||^2-+-\\mu)-}_\\text{regularization}-$$-5.1.1\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-4\"> \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} </script></a></span></li><li><span><a href=\"#$$r_{ui}-=-\\mu-+-U_u.I_i-$$\" data-toc-modified-id=\"$$r_{ui}-=-\\mu-+-U_u.I_i-$$-5.1.2\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-5\">r_{ui} = \\mu + U_u.I_i </script></a></span></li><li><span><a href=\"#STEPS:\" data-toc-modified-id=\"STEPS:-5.1.3\">STEPS:</a></span><ul class=\"toc-item\"><li><span><a href=\"#(1)-Model-definition\" data-toc-modified-id=\"(1)-Model-definition-5.1.3.1\">(1) Model definition</a></span></li></ul></li><li><span><a href=\"#(TODO)-Just-to-make-sure-you-were-following:-complete-the-following-forward-method\" data-toc-modified-id=\"(TODO)-Just-to-make-sure-you-were-following:-complete-the-following-forward-method-5.1.4\">(TODO) Just to make sure you were following: complete the following <code>forward</code> method</a></span><ul class=\"toc-item\"><li><span><a href=\"#(2-4)-full-train-loop\" data-toc-modified-id=\"(2-4)-full-train-loop-5.1.4.1\">(2-4) full train loop</a></span></li></ul></li></ul></li><li><span><a href=\"#(Your-turn)-Koren-2009-model:\" data-toc-modified-id=\"(Your-turn)-Koren-2009-model:-5.2\">(Your turn) Koren 2009 model:</a></span><ul class=\"toc-item\"><li><span><a href=\"#$$-\\min\\limits_{U,I}\\sum\\limits_{(u,i)}-\\underbrace{(r_{ui}----(I_i^TU_u-+-\\mu+-\\mu_i+\\mu_u))^2}_\\text{minimization}-+-\\underbrace{\\lambda(||U_u||^2+||I_u||^2-+-\\mu--+-\\mu+-\\mu_i+\\mu_u)-}_\\text{regularization}-$$\" data-toc-modified-id=\"$$-\\min\\limits_{U,I}\\sum\\limits_{(u,i)}-\\underbrace{(r_{ui}----(I_i^TU_u-+-\\mu+-\\mu_i+\\mu_u))^2}_\\text{minimization}-+-\\underbrace{\\lambda(||U_u||^2+||I_u||^2-+-\\mu--+-\\mu+-\\mu_i+\\mu_u)-}_\\text{regularization}-$$-5.2.1\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-15\"> \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} </script></a></span></li><li><span><a href=\"#$$r_{ui}-=-\\mu-+-\\mu_i-+-\\mu_u-+-U_u.I_i-$$\" data-toc-modified-id=\"$$r_{ui}-=-\\mu-+-\\mu_i-+-\\mu_u-+-U_u.I_i-$$-5.2.2\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-16\">r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i </script></a></span></li><li><span><a href=\"#TODO:\" data-toc-modified-id=\"TODO:-5.2.3\">TODO:</a></span></li><li><span><a href=\"#(TODO)-Here,-train-loop-stays-the-same,-you-only-have-to-change-the-model\" data-toc-modified-id=\"(TODO)-Here,-train-loop-stays-the-same,-you-only-have-to-change-the-model-5.2.4\">(TODO) Here, train loop stays the same, you only have to change the model</a></span></li></ul></li><li><span><a href=\"#Pytorch's-keras:-Pytorch-Lightning\" data-toc-modified-id=\"Pytorch's-keras:-Pytorch-Lightning-5.3\">Pytorch's keras: Pytorch-Lightning</a></span></li><li><span><a href=\"#Let's-try-with-the-same-but-different-Koren-2009-model:\" data-toc-modified-id=\"Let's-try-with-the-same-but-different-Koren-2009-model:-5.4\">Let's try with the same but different Koren 2009 model:</a></span><ul class=\"toc-item\"><li><span><a href=\"#$$r_{ui}-=-\\mu-+-\\mu_i-+-\\mu_u-+-U_u.I_i-$$\" data-toc-modified-id=\"$$r_{ui}-=-\\mu-+-\\mu_i-+-\\mu_u-+-U_u.I_i-$$-5.4.1\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-17\">r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i </script></a></span></li><li><span><a href=\"#$$-\\min\\limits_{U,I}\\sum\\limits_{(u,i)}-\\underbrace{(r_{ui}----(I_i^TU_u-+-\\mu+-\\mu_i+\\mu_u))^2}_\\text{minimization}-+-\\underbrace{\\lambda(||U_u||^2+||I_u||^2-+-\\mu--+-\\mu+-\\mu_i+\\mu_u)-}_\\text{regularization}-$$\" data-toc-modified-id=\"$$-\\min\\limits_{U,I}\\sum\\limits_{(u,i)}-\\underbrace{(r_{ui}----(I_i^TU_u-+-\\mu+-\\mu_i+\\mu_u))^2}_\\text{minimization}-+-\\underbrace{\\lambda(||U_u||^2+||I_u||^2-+-\\mu--+-\\mu+-\\mu_i+\\mu_u)-}_\\text{regularization}-$$-5.4.2\"><div class=\"MathJax_Display\" style=\"text-align: center;\"></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-18\"> \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} </script></a></span></li></ul></li><li><span><a href=\"#(TODO)-Complete-the-code\" data-toc-modified-id=\"(TODO)-Complete-the-code-5.5\">(TODO) Complete the code</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-5.6\">Train the model</a></span></li><li><span><a href=\"#Still-got-time-?\" data-toc-modified-id=\"Still-got-time-?-5.7\">Still got time ?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 4 : Pytorch and Recommenders (~1h)\n",
    "\n",
    "In this practical session, we dive a little more into [pytorch](https://pytorch.org/docs/stable/index.html) and propose to re-implement two classical matrix-factorization models with this neural network toolkit.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- (a) See a bit of simple pytorch (~5min)\n",
    "- (b) Discover the \"autograd\" part of pytorch to build a simple baseline (~20min)\n",
    "- (c) Discover the \"nn\" part of pytorch to build a simple matrix factorization algorithm (~20min)\n",
    "- (d) Learn to use a high level framework for pytorch (kind of \"KERAS\" like) to build more complicated algorithms (~15min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:22.731608Z",
     "start_time": "2019-12-13T18:00:22.728590Z"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install torch torchvision pytorch-lightning --upgrade\n",
    "#! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:25.239588Z",
     "start_time": "2019-12-13T18:00:22.995289Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### Tensors : the main unit\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "\n",
    "## Some useful functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize an empty 4x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:25.268419Z",
     "start_time": "2019-12-13T18:00:25.259414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [1.8754e+28, 6.3455e-10]])\n"
     ]
    }
   ],
   "source": [
    "x_empty = torch.empty(4, 2)\n",
    "print(x_empty)  #Tensor is not initialized => contains gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a 3x2 tensor filled with zeros of type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:26.183929Z",
     "start_time": "2019-12-13T18:00:26.177930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.zeros(3, 2, dtype=torch.long)\n",
    "print(x0) #Tensor has only zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a tensor of size 2 with (0 => 5.5) and (1 => 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:27.068912Z",
     "start_time": "2019-12-13T18:00:27.062938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([5.5, 3])\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:27.729106Z",
     "start_time": "2019-12-13T18:00:27.724106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor(np.array([5.5, 3])) #also works with numpy arrays\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create random 5x3 and 3x5 tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:28.835033Z",
     "start_time": "2019-12-13T18:00:28.828016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0056, 0.0296, 0.6323],\n",
      "        [0.4728, 0.9621, 0.8071],\n",
      "        [0.8156, 0.0695, 0.4625],\n",
      "        [0.3693, 0.6671, 0.6609],\n",
      "        [0.7850, 0.9297, 0.6266]])\n",
      "tensor([[0.7018, 0.1716, 0.6126, 0.2805, 0.7001],\n",
      "        [0.7777, 0.8684, 0.3495, 0.9277, 0.2430],\n",
      "        [0.4349, 0.5901, 0.5866, 0.2413, 0.0629]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(3,5)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing works just like numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:29.343761Z",
     "start_time": "2019-12-13T18:00:29.334218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0296, 0.9621, 0.0695, 0.6671, 0.9297])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1] #The 2nd column (indexing starts at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:29.647845Z",
     "start_time": "2019-12-13T18:00:29.641846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4728, 0.9621, 0.8071],\n",
       "        [0.3693, 0.6671, 0.6609]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,3],:] # the 2nd and 4th row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:29.990619Z",
     "start_time": "2019-12-13T18:00:29.985620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor([1])\n",
    "print(scalar.item()) # Gets the value when tensor is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### know the size of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:30.926728Z",
     "start_time": "2019-12-13T18:00:30.921669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size() ## equivalent to x.shape in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:32.280931Z",
     "start_time": "2019-12-13T18:00:32.273939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0056, 1.0296, 1.6323],\n",
       "        [1.4728, 1.9621, 1.8071],\n",
       "        [1.8156, 1.0695, 1.4625],\n",
       "        [1.3693, 1.6671, 1.6609],\n",
       "        [1.7850, 1.9297, 1.6266]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+1        # same as x.add(1)\n",
    "x.add_(1)  # inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:32.729990Z",
     "start_time": "2019-12-13T18:00:32.723023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2164, 2.0300, 1.9335, 1.6311, 1.0568],\n",
       "        [3.3455, 3.0231, 2.6481, 2.6693, 1.6214],\n",
       "        [2.7420, 2.1034, 2.3440, 1.8543, 1.6229],\n",
       "        [2.9798, 2.6628, 2.3958, 2.3313, 1.4680],\n",
       "        [3.4609, 2.9420, 2.7222, 2.6833, 1.8208]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x,y) # same as x @ y or np.dot(x.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to understand:\n",
    "\n",
    "Pytorch can be a drop-in replacement for numpy. It behaves mostly the same and the API is close.\n",
    "\n",
    "\n",
    "### There are many more creation/operation ops:\n",
    "\n",
    "=> You can have a look at the [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's interesting beyond the \"numpy replacement\": autodiff !\n",
    "\n",
    "Pytorch has Automatic differentiation: You only have to compute a loss function to obtain gradients automatically. How it works is detailed [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd)\n",
    "\n",
    "### Let's do 1d-linear regression with the vanilla autodiff !\n",
    "\n",
    "#### (First) we need fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:34.084157Z",
     "start_time": "2019-12-13T18:00:33.938152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26358e868c8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3RU17n38e+eUe8S6g0JVKgCgRBgDC7ghmsSFxybOI5LulvKdZL3pic3NzfXN7aT2CbucS/EFTuxwTY2BoEQAklISEK9od67Zr9/zDBISAJZbTSj57MWC3TOmZl9fKSfj56zi9JaI4QQwv4YbN0AIYQQ4yMBLoQQdkoCXAgh7JQEuBBC2CkJcCGEsFNO0/lhgYGBOiYmZjo/Uggh7N7BgwfrtdZBp2+f1gCPiYkhPT19Oj9SCCHsnlKqdKTtUkIRQgg7JQEuhBB2SgJcCCHslAS4EELYKQlwIYSwUxLgQghhpyTAhRDCTkmACyHEFDpe184D/z5GbWv3pL+3BLgQQkyhPYX1PLSrkD7T5K+9IAEuhBBT6FBZM0HeroT7uk36e0uACyHEFMosbyY5yg+l1KS/twS4EEJMkaaOXorrO1ge7Tcl7y8BLoQQUySzohmA5VES4EIIYVcyy5pRCpIiJcCFEMKuZJY3kxjijZfr1MzcLQEuhBBTQGtNZnnzlJVPQAJcCCEmxacFddz7cia9/SYAius7aOnqm9IAn9YVeYQQwhH19pv4yfYsKpq6SIr05dZ1sWSWWx5gTlEPFJA7cCGEsGrp6qO7b+ALv+6lA2VUNHUR6e/OgzsLaOnq41BZM54uRuKDvaegpWZnDXCl1JNKqVqlVPagbQFKqQ+UUgWWv/2nrIVCCDFNrn90L794M+cLvaazt5+HdhaSGhvAY1tX0tLVx98+KiSzvJmkSD+MhskfwHPSWO7AnwYuPW3b/cBOrXU8sNPytRBC2K2Wzj6OnWjjw9wTmL7AvCVPf15CfXsPP74kkcXhvnw5OZKn9pSQW906peUTGEOAa613A42nbb4aeMby72eAaya5XUIIMa1yqlsAaOjoJbemdUyvaenq49GPj3PhgmBSYgIA+OElCSgF/SY9pQ8wYfwPMUO01tUAWutqpVTwaAcqpe4E7gSIjo4e58cJIcTUyqk8FdqfFdSzONx3xONqW7t550g1WZUtpJc20trdzw8uTrDuD/N155sb5vHIJ8dZET211eUp74Witd4GbANISUmZ/PkUhRBiEuRUtRDq44a3mxOfFdbzzfPmDzumob2Hrzz6OeWNXYT4uJIU6cd9FyUMC/t7NiVw/aoogrxdp7TN4w3wE0qpMMvddxhQO5mNEkKI6ZZT1cricB+i53jwQloZ3X0DuDkbrft7+gf41nMHOdHaw6vfWssqS8lkJAaDItLfY8rbPN5uhG8Bt1j+fQvw5uQ0Rwghpl9X7wDH69pZHO7D+vhAevpNpJc0Wfdrrfnp9mwOlDTxp+uWnTG8p9NYuhG+COwFEpVSFUqp24A/ABcppQqAiyxfCyGEXcqracWkYVG4L6tj5+BsVHxaWGfd/9juIl7PqODujfFctSzchi0d6qwlFK31jaPs2jjJbRFCiEl1uLyZpRG+GM7SFzunyvwAc3G4D56uTiRH+/NZQT1cBrvz6/jj+3lcnhTGPZvip6PZYyYjMYUQDuloVStX/3UPb2RWnvXYnKpWfN2difR3B2B9XCA5Va0cLm/mrpcOER/szf9cmzQlq+pMhAS4EMIh7S9uAOCzwvqzHnu0qoVFYT7WgF6fEATATY+nYTJpHtu6Eg+XmTd1lAS4EMIhZZSZJ5NKKzp9HOJQ/QMm8mraWBzuY922NMIXX3dnOnr7eXBLMjGBnlPa1vGaef9LEUKISZBR1oSTQVHZ3EV5YydRASN36zte10FPv4nFEacC3GhQ/OiSRJwMigsWjDpO0ebkDlwI4XBqW7upaOriS8kRAOwrahj12Jwq8xD60wfj3LxmLltSZ/bocQlwIYTDySgz9+HekhpFgKcLacWjl1FyqlpxdTIwb4aWSc5EAlwI4XAyyppxMRpYEuFLakzAWe/AF4T54GS0vzi0vxYLIcRZZJQ2sSTCB1cnI2vmBVDRZK6DD9bdN8BDOws4WNrEssiRJ66a6eQhphDCofT2mzhS2cLX1swFYM38OQCkFTdaH2Tuzq/j529mU9LQyeVJYdy9cWYN0BkruQMXQjiUnKoWevtNrJhrnso1IdgbPw9n0ixllLcPV3HLU/sxKMU/bkvlr19dwRyvqZ01cKrIHbgQwqGc7P+90hLgBoNidWwA+4ob+PhYLfe+nMmquQE8841U3F2MZ3qrGU/uwIUQDiWjrIkIP3dCfNys29bMm0N5Yxff/MdBEkK8efzrKXYf3iABLoRwMBmlTSSfthblmnnmOni4nzvP3paKj5uzLZo26aSEIoSYcdp7+tmVV8t7WdWYtObRm1eOaSKp8sZOqlu6hy1ltiDUmwe3LGd17BwC7bTePRIJcCHEjLI9o4L7t2fR22/Cw8VIZ+8AGWXN1pr2aFo6+/j28wdxcTJwXmLQkH1KKa5eHjGVzbYJKaEIIWaUZz4vIcrfnVe+uZa9P9mIq5OBtw9XnfE1rd19fO3JNPJr2nns5pXMD/KaptbaltyBCyGmRU//ALc/k05ylB/3XZw44jGt3X1kVbbw3QviSI01L1t24YJg3jlSzf+7fKF1tGRNSzdP7SnGyahwdzbyYW4tOVWtPHrzyhk9+dRkkwAXQkyLX7yZw6cF9RyvbefeixJGrGkfKG7EpGGt5aEjwJXLwnkvu4Z9RY2cGx8IwG/eOcqO7GoMSjFg0rg5G/jLV5PZtChk2s5nJpAAF0JMuRfSynjpQDnxwV4U1LZT0tBJ7AiTR+093oCL0WAdhAPmO3AvVyfeOlzJufGBHCxt4t2sau7ZFM89mxLoGzBh0hpXJ/vvFvhFSQ1cCDGlMsqa+MVb2WxICOKRm1cCo6+Ss7eogeRoP9ycT4Wxm7ORixeF8H52DT39A/zu3aMEe7ty54Z5ADgbDbMyvEECXAgxCbTW/H13ER/l1aK1tm57+3AV33j6AGG+7jy0ZTnzgzwJ93Xj8xECvLmzl6PVraydP2fYviuXh9Pa3c9Pt2eTUdbMfRclzMglzqab/BcQQkzY8bp2frcjF4DkaD++fd583sisZEdWDcsiffnzlmT8PFwAWBcXyAe5JxgwaYyDVotPK25En1b/PuncuED8PZx5PaOCxBBvrkuJmp4Tm+HkDlwIMWHpJeYFFO7aGE9NSzd3/uMgHx6t5ceXJvL6t88ZUu9eFxdIc2cfR6tah7zH3uMNuDoZWH7aKEowl0kuWxoGwP2bFwwJ/tlM7sCFEBOWXtpEgKcL926K5zvnz+fD3BMsCPUhLnh4f+xz4sx32HuO17N00Dzc+4oaSInxH7Wefc/GeFJjAjg/IWjE/bOR3IELISbsYGkTK6L9UUrh5mzkiqTwEcMbINjbjYQQL/YMqoM3dvSSV9M2YvnE+jofN65JjhjTkPrZQgJcCHFGXb0DPPlZMb39phH317f3UFzfQUrMmYe6D7YuLpADJY109w0AWOfqHukBphidBLgQ4oxez6jg1+8cZVde7Yj7M0rN9e+Us8xVMti6+YF095nIKGuiuqWLF/aX4eFiJClyeP1bjE5q4EKIM/pXTg0A+4sbuXRJ6LD9B0ubrAsIj9XqeQEYDYr/90Y25Y2dmDTcvTEeZztcWNiWJhTgSql7gdsBDWQBt2qtuyejYUII22vp6mPvcXN5Y3/JyCu7p5c2sTTSd8jgm7PxdnNmzbwA0kua+GpqNHdsmEekv8ektHk2GXeAK6UigLuARVrrLqXUK8AW4OlJapsQwsY+PlZLv0mzPj6QPYX1tHb3DVkMobtvgKyKFm5dF/OF3/uxrSkMmDS+7o6xuIItTPT3FSfAXSnlBHgAZ57zUQhhV/6VU0Owtyvf3DAfk4aDlv7eJ2VXttA7YBoyd8lYebk6SXhP0LgDXGtdCfwJKAOqgRat9b9PP04pdadSKl0plV5XVzf+lgohplV33wAfH6vjokUhrJzrj5NBkVbcOOSYdMsDzLMttiCmxrgDXCnlD1wNxALhgKdS6ubTj9Nab9Nap2itU4KCpAO+EPZiT2E9nb0DXLw4FHcXI0mRvuwvHloHTy9pIjbQ06GWKbMnEymhbAKKtdZ1Wus+YDtwzuQ0Swhha//KqcHb1ck6uGb1vDkcqWihq9fcd1trTUZZk9x929BEArwMWKOU8lDmoVEbgdzJaZYQwpYGTJoPc2u5YEEwLk7mmEiNDaDfpDlUZi6bfHD0BI0dvdaVc8T0m0gNPA14DcjA3IXQAGybpHYJIWxoT2E9jR29XLL4VL/vlLn+GBTsK26ktq2b+7dnsSjMh6uXh9uwpbPbhPqBa61/AfxiktoihJgBGjt6uf/1I0T6u3PBglPPrbzdnFkc7ktaUQNHKprp6OnnoRuXz9rFFGYCGYkphLAaMGnueTmT+vZeXv/2OcMWTUiNDeCJz4oB+PXVi4kL9rZFM4WFjFsVQlg9vKuA3fl1/PKqxUOmej3pZL37/MQgtq6ZO93NE6eRO3AhHNRnBfU4GRVrzjBF62D7ihp4cGcBX14RwY2pI694c15CEHdvjGfr2rkyresMIHfgQjgYk0nzv/8+xs1PpPH9Fw8xYNJD9ufVtPLGocphr3vgg3xCfdz43TVLRw1nN2cj916UIP2+ZwgJcCEcSFt3H3f+4yAP7yokKdKXurYe0kuGjp789dtHueflTDLKTg2LP1jaxP7iRm5fPw93F3koaS8kwIVwEFprbnsmnY+O1fKrqxbz4h1rcHUysCOr2npMRVMney2LJ/z67aPWFeQf/eQ4vu7ObFkliwXbEwlwIRzE58cb2F/cyM+vWMQt58Tg6erEBYnBvJddg8lSRvlnRiXaMvd2Znkzb2ZWUVjbxgdHT3DL2rl4uspjMXsiAS6Eg3h4VwEhPq7cMOguenNSGLVtPaSXNqG1ZvuhSlbHBnD3xniWRvjyh/fy+POHBbg5G7jlnBjbNV6MiwS4EA4gvaSRfUWN3Llh/pCFFTYuCLaWUTLKmiiu7+ArKyMxGBQ/v3IRNa3dvHOkmhtSopgjDybtjgS4EDPYyUV/z+YvHxUS4OkyrPufp6sT5ycGsSOrmlfTK3B3NrJ5aRgAq2ICuCIpDCeD4vb18ya97WLqSYALMUOdaO1m5W8+GLHL32BZFS18fKyO286NHTZyEmDzUnMZ5dWDFVy2JBSvQXXu//5KEm9971yiAmQ5M3skAS7EDPVWZhUdvQO8kl4+6jFaax7cmY+3mxNb1448MnLjwhBcnAwMmDRfWRk5ZJ+nqxOLwn0mtd1i+kiACzFDvZFpvvPeV9RAbdvIa4U/uaeED3Nr+c75cUPWqhzMy9WJixeFEB3gYZ3bWzgGCXAhbKy2tZutT6RRUt9h3VZY20ZOVSs3pkZj0vBeVs2w1+3Or+N37x7lksUhfHPDmWvYf7w2iTe+uw6DQYa/OxIJcCFs7PPjDXxaUM9v3jlq3fZmZhUGBfdeFE9iiDdvHx66XnhxfQffeyGDhBBvHrh++VmD2cPFiQBPlylpv7AdCXAhbCz/RBsAO/Nq2Z1fh9aaNzOrWBcXSLC3G1ckhZFe2kRVcxdgHi5/x7PpGA2Kv38tRQbfzGIS4ELYWP6JdmLmeBAd4MFv3z1KemkTZY2dXLXMvNLNFZa/d2RVYzJp7n05k+L6Dv560wrpPTLLyf+6hbCxgto2Fof7cNWycL71XAb3vJSJq5OBS5eYlzOLDfRkSYQPbx+uorW7nw9za/nllYs4Z36gjVsubE3uwIWYJgMmTXlj55BtXb0DlDV2Eh/szSWLQ1kdG0BlcxebFobgPahXyZVJ4RyuaOGhnQVctzJShr0LQAJciGnzz0OVXPCnj6m01LIBjte1ozUkhHijlHl4u4eLkS2njai8PCkMpWBZlB+/uWaJLKYgACmhCDFtDpU10W/S7DveYB1QU1BrfoCZEOIFwOJwX7J/ecmwXiWR/h5s//Y5zAvyGjLXiZjd5A5ciGmSW90KwP7iUwss5J9ox9moiAn0tG4brUtgcrQ/vu4jD9YRs5MEuBDTwGTSHKsx323vH7RCTsGJNmIDPXE2yo+i+OLku0aIaVDe1ElH7wBxwV4U13dYh8bnn2gnPsTbxq0T9koCXIhpcLJ8cotlwqkDxU109Q5Q3tRJQrAEuBgfCXAhpsHR6jYMCq5JjsDd2cj+4gYKa0/2QPGydfOEnZIAF2Ia5FW3EhvoibebMyvn+pNW3GgdQi8lFDFeEuBCjFNhbRuHy5vHdGxuTSsLw8zzbqfGBnDsRBvppU3mHihzZDi8GJ8JBbhSyk8p9ZpSKk8plauUWjtZDRNiprv35cPc+vQBevtNQ7a/tL+MG7ftY8CyEnxbdx/ljV3WAF8VE4DW8FZmJfMCvXCSHihinCb6nfMg8L7WegGwDMideJOEmPlKGzrIqmyhsaOXXXm11u0DJs3DuwrZW9TAx8fM2/Ms3QcXhplLJcnRfjgbFR29A8RL/VtMwLgDXCnlA2wAngDQWvdqrcf2+6QQdu7drGoA/DyceXXQkmef5NdS2dyF0aB4bl8pYK5/A9Y7cDdnI8si/QDzEHohxmsid+DzgDrgKaXUIaXU40opz9MPUkrdqZRKV0ql19XVTeDjhJg5dmRVszzKjxtTo/noWC0nWs39up/fV0aQtyt3bpjHx/l1lDd2crS6DT8PZ0J93KyvT40NAKQHipiYiQS4E7ACeERrnQx0APeffpDWepvWOkVrnRIUFDSBjxNiZiht6CC7spUrksK4PiUKk4btGZVUNHWy61gtW1ZFsXXNXBTwwv4ycqtbWRjqM2QCqksWhxLq48aKaH/bnYiwexOZzKoCqNBap1m+fo0RAlwIR3OyfHLZ0jAi/NxJjQng1fRy2rr7UMCW1GjC/dy5cEEIrxwop7N3YNjsgsui/Nj30402aL1wJOO+A9da1wDlSqlEy6aNwNEzvEQIu1HV3EVX78CI+949Uk1ytB8Rfu4AXJcSSVF9B098VsyFC4Kt229eE01DRy9dfQPW+rcQk2mivVC+DzyvlDoCLAd+P/EmCWFbHT39XPbgp2zZtpee/qEhXlLfQU5VK5cvDbNu27w0DE8XIz39Jm5aM9e6fUN8ENGWJc8WSYCLKTChANdaZ1rq20la62u01k2T1TAhbOWNzEpauvo4XNHCr98e+kvlyfLJ5kEB7unqxHUpUcQHe7Eh/tRzHoNBccf6WAK9XIgLloeVYvLJgg5CDKK15rl9ZSwM82FDfCCP7S5iRbQ/Vy0P58nPinl4VwGrYvwJt5RJTvr5FYswaY3xtLm8t66N4aur5w7bLsRkkAAXDq+orp0btu1jcbgPly8N4+JFofh6jLwwQkZZM7nVrfzuS0u4ISWKwxXN/OyNLLbtLuLYiTY2LQzht9csGfY6g0FhYOSQlvAWU0XG8AqH90JaGc2dvRTWtvOj146w6vcfcrC0ccRjn99XiperE9csj8DJaODhG1fg5+5CS1cfj21dyeO3pBDq6zbia4WYbnIHLhxab7+J7Ycq2bQwhL/dtIIjFS3c/HgarxyoYOXcgCHHNnX08k5WNTekROHpav7RCPJ25YP7NuBsNMhalGLGkQAXDm1n7gkaO3q5flUUSimWRfmxITGInXm1mEx6yPqTrx4sp7ffxM2DepIAeLvJOpRiZpISinBoL6eXE+brNqR3yKaFwdS393CkssW6zWTSvJBWxqoYfxJDZX4SYR8kwIXDqm7pYnd+HdeujBzyIPH8hGAMCnblnrBu+zi/lpKGTraujbFBS4UYHwlw4bBeS6/ApOG6lUOHsft7upAyN4APc09NA/vUnhJCfdy4bEnodDdTiHGTABcOyWTSvHqwgrXz5hA9woo3GxcGc7S6larmLvJPtPFpQT1b187FWRZXEHZEvluFw+nuG+D3O3Ipa+zkhlVRIx6zcWEwADvzanlqTwmuTgZuTI2ezmYKMWHSC0U4lMzyZn746mEKa9u5MTWKy5PCRjxufpAXc+d48M+MCo5Wt/Kl5AgCPF2mubVCTIwEuHAYewrr2fpEGiE+bjzzjVTOSxh9/nmlFBsXhPDknmIAvr4uZppaKcTkkRKKcBjbdhcR7O3Gv+7dcMbwPmmTpYyyLm4OC0JltkBhfyTAhUMoa+hkd0EdW1Kj8BnjwJtVsQFcszycH16cePaDhZiBpIQiHMIL+8swKMWWVWN/EOlsNPDnLclT2CohppbcgQu719M/wCvp5WxcECwTTYlZRQJc2J3efhMtnX3Wr9/PrqGxo3fYHCZCODopoQi7obXmvewa/vBeHhVNndyYGs19FyXw/L4y5s7x4Ny4QFs3UYhpJQEu7EJ2ZQu/fCuH9NImEkO8uT4lipcOlPNWZhVtPf385LIFQ2YWFGI2kAAXM1pP/wAP7yzkkU+O4+/hwn99eSnXp0RhNChuXx/Lb9/NJauihWtXRtq6qUJMOwlwYXM/fu0w6aVNLAzzYVGYD0HervQPaPoGTLyQVsaxE21cuzKS/7x80ZCl0OKCvXn61lS01igld99i9pEAFzZV19bDawcrmB/kxZGKZt49Uj1kf6iPG09+PYULF4SM+h4S3mK2kgAXU+pQWRPxId54uY78rfZ+Tg0mDX/56goSQ71p7e6jpbMPZ6MBJ6PC191ZZggUYhTykyGmTH17D9c+updtnxwf9Zh3j1QRF+xFQogXAD5uzkQFeBDq60agl6uEtxBnID8dYsqkFTUyYNJ8frxhxP21bd2kFTeyeWmYlEGEGAcJcDFl0orNwX24opmu3oFh+/+VXYPWcMUoU74KIc5MAlxMmX1FDXi5OtE3oDlU1jRs/ztHqi3lE1lEWIjxkAAXk2Lw0HaAhvYe8k+0s3XtXAwK9hU3Dtlf29rN/pJGLl8qd99CjNeEA1wpZVRKHVJKvTMZDRL2p6iunRW//YD3sk51AdxvCexNC4NZHO5LWtHQOvj7OebyyWgr5gghzm4y7sDvBnIn4X2EndpneVi57dMi67a04kbcnY0sjfBjdWwAh8qb6e47VQd/+3AV8VI+EWJCJhTgSqlI4HLg8clpjrBHGZb69qGyZo5UNAPm+vfKuf64OBlIjQ2gt9/E4XLzvv3FjRwoaeK6FBn+LsRETPQO/M/AjwHTaAcope5USqUrpdLr6uom+HFiJsooa2J1bACeLkae/ryEpo5e8mraWDMvAIDU2ACUMt+Va635n3/lEeTtytY1MbZtuBB2btwBrpS6AqjVWh8803Fa621a6xStdUpQ0NnXKRT2pbmzl6K6DjYkBHHtykjeOVzNjmxzLXz1vDkA+Hm4kBjiTVpxA5/k13GgpIm7LozD3cVoy6YLYfcmMpR+HXCVUmoz4Ab4KKWe01rfPDlNE/bgkKUskhztZ14Nfm8pf3gvDzdnA0mRvtbj1sybw0sHymjuPEakvzs3fIGlz4QQIxv3HbjW+ida60itdQywBdgl4T37HCprxqBgWaQf84O82JAQRFt3Pyui/XF1OnWHvTo2gO4+EzlVrdyzKQEXJ+nBKsREyU+RmJBDZU0khvrgaZms6uvnmJc1Wx07Z8hxqbHmenhcsBdfSo6Y3kYK4aAmZTZCrfXHwMeT8V7CfphMmsyyZq5cHm7ddn5CML+9ZgmXLQkdcuwcL1d+tnkhK2P8McrKOUJMCplOVgzT0tnHgZJGLlgQfMawLaxrp63HXC45yWBQoy4ufMeGeZPeViFmMymhiGEe3lXA7c+mc/1jeymsbQOgvaefZz4v4bvPZ3CitRvAOr9JcrSfzdoqxGwmd+BimI/z64iZ48HxunY2P/gZFy0O4ZNjdbT39GNQUFzfwSvfWktGaTO+7s7MC/S0dZOFmJUkwMUQVc1dFNa287PNC7kmOYJfvpXDh0dPcNmSUG45J4aWrj5ueyadbz93kKrmLpKj/WQubyFsRAJcDPFpgXm07IaEIIK8XfnrTSuGLRr8u2uWcP/2LACuXi49SoSwFamBiyF259cT6uNmXeIMhi8avCU1mu9fGAec6h4ohJh+cgcurAZMms8K67l4UchZyyL3XZTAVcvCiQv2OuNxQoipIwEurI5UNNPS1cf6hLPPWaOUIl6mghXCpqSEIqx259ejFKyPC7R1U4QQYyABLqx2F9SRFOGLv6eLrZsihBgDCXABQEtXH5nlzayPlyl/hbAXUgO3MyaT5uFdhVQ0ddI3YKLfpPnGubFDhrOPx+eF9QyYNBvGUP8WQswMEuB2Zm9RA//3YT6BXq54uhqpb+uhsLadHXetxzDOSaI+OlbLf76ZQ4CniwyLF8KOSIDbmZcPlOPj5sRn/3EBbs5G3sys5O6XMnknq5qrloWf9fX/PFTB54UNJIZ6szDMhw+OnuDpz0tIDPHmz1uW42yUqpoQ9kIC3I60dPbxfk4NW1ZF4eZsXizhyqRwHvn4OA/8+xiXLQk9YwAX13fwH69noYCeg6eWMb11XQz/cekC63sKIeyDBLgdeSOzkt5+E9enRFm3GQyKH1ycyB3PpvP6wQq2pI68VJnWmv98IxtXo4GdPzgPg0GRW92Kr7szSZFSNhHCHsnvyzOUyaR5L6uajp5+67aXD5SzONyHJRG+Q47dtDCY5Gg/HtxZQHffwIjv92ZmFZ8V1vPjSxMJ9nEj0MuV9fFBEt5C2DG5A5+hdmRX870XDrEs0pcnv76K6pZujla38purFw87VinFjy5J5Kt/T+O6R/cSG+hJkLcr8cFerJ43B38PZ3777lGWRfnx1dUjL7YghLA/EuAz1JuZVfi6O5NX08Z1j+4lIcQbVycDV40y+9858wO5d1MCnxbUcbiimdrWHrosd+PuzkZ6B0w8840lspyZEA5EAnwGauns4+NjtdyyNoZLloRy29MHKKrv4Jrl4fi6O4/6urs3xXP3pnjAXPM+XtfOvqJG9hc3snKuP4vDfUd9rRDC/kiAz0DvZVfTN6C5enkESyN9eeVba/ndu7l887z5Y34PpRRxwd7EBXuPukalEMK+SYDPQG8driI20Hl2n+MAAAyfSURBVJMlET4ALAj14R+3rbZxq4QQM430QplhTrR2s7eogauWhctSZUKIM5IAn2HePlyF1nDV8rOPqhRCzG4S4NNMa80zn5fwrX8cHLHP9luHq1ga4cv8IFnpRghxZhLgU0hrTVfvqZCua+vh1qcP8Iu3cng/p4Z/HqoccnxRXTtHKlrGNKeJEELIQ8wpoLXm/ewaHvggn4LadoK8XYkN9OR4bTvtPf38+urFvJJezt8/LeKGlCjrLIIP7yrE1cnA1VI+EUKMgQT4JEsvaeSXb+eQXdnK/CBP7t4YT1VzFyUNHSSGevPLqxaTEOKNr7szd7+Uya68WjYtCiG7soV/HqrkO+fPJ9jHzdanIYSwA+MOcKVUFPAsEAqYgG1a6wcnq2H26P3sau56MZMgb1f+dN0yvpQcMerIx81Lw/jj+8fY9mkRGxcG81/v5eLv4cy3zh97X28hxOw2kRp4P/ADrfVCYA3wXaXUoslplv15cX8Z33k+gyURPrx717lcuzLyjMPWnY0Gbl0Xw/7iRh7eVciewgbu2hiPj9voIy2FEGKwcQe41rpaa51h+XcbkAuMPFGHg3vm8xJ+sj2L8xKCeP72Nfh5jG1R4C2p0Xi7OfHAB/lEB3hwk0w0JYT4AialF4pSKgZIBtJG2HenUipdKZVeV1c3GR83owyYNA/tLGBd3By2fS0Fd5exL4rg5epkDe0fXZKIi5N0ChJCjN2EH2IqpbyA14F7tNatp+/XWm8DtgGkpKToiX7eTLO/uJGGjl5uWj13XMuRff/COJZG+LJ5aegUtE4I4cgmdMunlHLGHN7Pa623T06TZq73sqrJrmwZui27GjdnA+cnjm81d09XJy5PCpNh80KIL2zcAa7MifMEkKu1fmDymjQz9Q+YuO+Vw9z90iEGTOZfJAZMmveya7ggMRgPF+mRKYSYXhO5A18HbAUuVEplWv5snqR2zTh5NW109Q1wvK6DHVnVABwsbaKurYfNS8Ns3DohxGw0kV4on2mtldY6SWu93PJnx2Q2zha01vz3+3nk1Qwt5x8qbwYgxMeVh3cVYDJpdmRV4+pk4IIFwbZoqhBilpNuD6cpru/gkY+P89RnJUO2HyprItDLlZ9uXkj+iXZ2ZFfzfnYN5yUE4eUq5RMhxPSTAD/NkQrzQ8pPC+rQ+lSnmcyyZpKj/bgiKZx5QZ785xvZ1LR2S/lECGEzEuCnOVxhLpVUtXRzvK4DgObOXorqO1ge5YfRoPj+hXE0dfbhYjSwcaGUT4QQtiEBfpojFS1E+LkD5rtwgExL/Ts52g+AK5PCSQjx4qJFIXjL0HchhI1IgA/SP2Aip6qFSxaHEhvoyacF9QAcKmvGoCAp0hzgTkYD//zOOv73+mW2bK4QYpaTp2+D5J9op7vPxLIoX/oGTLyeUUFvv4lD5c0khHgPeVjpKQ8uhRA2Jnfggxyx1L+TIv1YHx9IZ+8A6aWNZJY1kRztb+PWCSHEULM2wFu6+vjO8wcpbeiwbjtc0YKPmxMxczxYO38ORoPi2c9Lae3uJznKz4atFUKI4WZtgH949AQ7smr4y65C67YjFc0si/JDKYW3mzMrov14P6cGOPUAUwghZopZG+C7LT1M3sysoratm+6+AY7VtJEU6Ws9Zn28eYIqb1cnWSVeCDHj2HWAlzZ0WLv4fREDJs3u/DpWzvWnd8DEc3tLOVrdSr9JW3uaAKyPDwRgebSfdeFhIYSYKeyyK8Wxmjb+9nEhbx+uwqThmuXh/OqqJfh6jK1PdnZlC02dfXxt7Vz8PZx5Lq0MD0uvkmWDAjwp0o95gZ5cKHOdCCFmILsL8Gf3lvDzN3PwcDFyx/p5uDob+etHhewrauSP1yaxIeHs83J/kl+HUnBuXCDB3m58+Pd9/O2jQoK9XQn1PbUivNGg2PXD86fuZIQQYgLsLsDfPlzFglBvXrxjDf6e5rUnNy0M5r5XDvO1J/dz30UJfO+CuDOWPHbn17E0wpc5Xq6s8XRhcbgPOVWtpMbOma7TEEKICbOrGrjWmrzqNlbFBFjDG8yljne+fy5fSo7ggQ/y+c7zGXT09I/4Hi2dfWSUNXGe5U5dKcXt62MBWDboAaYQQsx0dnUHXtHURVtPPwvCvIftc3M28sD1y1gc7sPvd+Sy+aFW1sUFsiDUm6RIP5Zb+nHvOV6PSWMNcIArksI5XtvBl1dGTtu5CCHERNlVgOfVtAGwINRnxP3mu+l5LAj14eFdBbx7pJoX0soAuGl1NL+4cjG78+vwdnOyBjqAs9HADy9JnPoTEEKISWRfAV5tXiUnMXT4Hfhg58YHcm58IFpralq7eXpPCY/tLiK3upWKpi7OjQvEaRwryAshxExiXwFe00Z0gMeYV8BRShHm685PNi8kKdKPH712mM7egSHlEyGEsFd2FeC5Na0sOMvd92guTwojLtiLF9JK2Zwkq+gIIeyf3dQRunoHKKnvYGHYyPXvsUgM9eZXVy/BRxZhEEI4ALsJ8ILaNkwaFo7QA0UIIWYjuwnwXMsDzNF6oAghxGxjRwHehruzkegAD1s3RQghZgS7CfC8mlYSQ71lVkAhhLCwiwDXWpNX0yb1byGEGMQuAvxEaw/NnX1S/xZCiEEmFOBKqUuVUseUUoVKqfsnq1Gny60xP8CcSBdCIYRwNOMOcKWUEfgrcBmwCLhRKbVosho2WF61eQ6Usw2hF0KI2WQid+CpQKHWukhr3Qu8BFw9Oc0aKre6lQg/d3zdZQCOEEKcNJGh9BFA+aCvK4DVE2vOyBaEeRPu5z4Vby2EEHZrIgE+Un8+Pewgpe4E7gSIjo4e1wd95/y4cb1OCCEc2URKKBVA1KCvI4Gq0w/SWm/TWqdorVOCgmQWQCGEmCwTCfADQLxSKlYp5QJsAd6anGYJIYQ4m3GXULTW/Uqp7wH/AozAk1rrnElrmRBCiDOa0HzgWusdwI5JaosQQogvwC5GYgohhBhOAlwIIeyUBLgQQtgpCXAhhLBTSuthY2+m7sOUqgNKx/nyQKB+EptjL2bjec/Gc4bZed5yzmMzV2s9bCDNtAb4RCil0rXWKbZux3Sbjec9G88ZZud5yzlPjJRQhBDCTkmACyGEnbKnAN9m6wbYyGw879l4zjA7z1vOeQLspgYuhBBiKHu6AxdCCDGIBLgQQtgpuwjw6Vo82ZaUUlFKqY+UUrlKqRyl1N2W7QFKqQ+UUgWWv/1t3dbJppQyKqUOKaXesXwdq5RKs5zzy5bpih2KUspPKfWaUirPcs3XOvq1Vkrda/nezlZKvaiUcnPEa62UelIpVauUyh60bcRrq8wesmTbEaXUii/yWTM+wKdz8WQb6wd+oLVeCKwBvms5z/uBnVrreGCn5WtHczeQO+jr/wb+z3LOTcBtNmnV1HoQeF9rvQBYhvn8HfZaK6UigLuAFK31EsxTUG/BMa/108Clp20b7dpeBsRb/twJPPJFPmjGBzjTuHiyLWmtq7XWGZZ/t2H+gY7AfK7PWA57BrjGNi2cGkqpSOBy4HHL1wq4EHjNcogjnrMPsAF4AkBr3au1bsbBrzXm6avdlVJOgAdQjQNea631bqDxtM2jXdurgWe12T7ATykVNtbPsocAH2nx5AgbtWVaKKVigGQgDQjRWleDOeSBYNu1bEr8GfgxYLJ8PQdo1lr3W752xOs9D6gDnrKUjh5XSnniwNdaa10J/AkowxzcLcBBHP9anzTatZ1QvtlDgI9p8WRHoZTyAl4H7tFat9q6PVNJKXUFUKu1Pjh48wiHOtr1dgJWAI9orZOBDhyoXDISS833aiAWCAc8MZcPTudo1/psJvT9bg8BPqbFkx2BUsoZc3g/r7Xebtl84uSvVJa/a23VvimwDrhKKVWCuTR2IeY7cj/Lr9ngmNe7AqjQWqdZvn4Nc6A78rXeBBRrreu01n3AduAcHP9anzTatZ1QvtlDgM+KxZMttd8ngFyt9QODdr0F3GL59y3Am9Pdtqmitf6J1jpSax2D+bru0lrfBHwEXGs5zKHOGUBrXQOUK6USLZs2Akdx4GuNuXSyRinlYfleP3nODn2tBxnt2r4FfM3SG2UN0HKy1DImWusZ/wfYDOQDx4Gf2bo9U3SO52L+1ekIkGn5sxlzTXgnUGD5O8DWbZ2i8z8feMfy73nAfqAQeBVwtXX7puB8lwPpluv9BuDv6Nca+BWQB2QD/wBcHfFaAy9irvP3Yb7Dvm20a4u5hPJXS7ZlYe6lM+bPkqH0Qghhp+yhhCKEEGIEEuBCCGGnJMCFEMJOSYALIYSdkgAXQgg7JQEuhBB2SgJcCCHs1P8H4g6KKl7FdfkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_x = np.arange(0,100,1)\n",
    "fake_y = np.arange(0,10,0.1) + np.random.rand(100)\n",
    "plt.plot(fake_x,fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Here, we won't split the data in train/val/test, this is just an example\n",
    "\n",
    "So, the linear model we want to learn is the following:\n",
    "$$f(x) =  wx+b $$\n",
    "The parameters to optimize are w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:34.997387Z",
     "start_time": "2019-12-13T18:00:34.993385Z"
    }
   },
   "outputs": [],
   "source": [
    "# First we create to tensor variables (which are our parameters)\n",
    "w = torch.tensor([1.],requires_grad=True) # We need to set requires_grad to True so the gradient can flow.\n",
    "b = torch.tensor([0.5],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:35.459311Z",
     "start_time": "2019-12-13T18:00:35.455339Z"
    }
   },
   "outputs": [],
   "source": [
    "# We define the f function:\n",
    "def f(x):\n",
    "    return (w*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:35.943296Z",
     "start_time": "2019-12-13T18:00:35.939303Z"
    }
   },
   "outputs": [],
   "source": [
    "# We define an error function (here the MAE)\n",
    "def error(pred,real):\n",
    "    return (pred-real).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:37.644405Z",
     "start_time": "2019-12-13T18:00:36.918437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "loss: 32.51134943962097\n",
      "w: 0.5049999952316284\n",
      "b: 0.4900013208389282\n",
      "----\n",
      "loss: 8.413745762109757\n",
      "w: 0.1048000305891037\n",
      "b: 0.4810025095939636\n",
      "----\n",
      "loss: 0.3336795634031296\n",
      "w: 0.09960001707077026\n",
      "b: 0.4808025360107422\n",
      "----\n",
      "loss: 0.34361385941505435\n",
      "w: 0.0949999988079071\n",
      "b: 0.4808025360107422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x263590d6248>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, we cycle through the data, optimizing the parameters with respect to the gradient of the error:\n",
    "plt.plot(fake_y)\n",
    "\n",
    "for epoch in range(4): # We cycle 4 times\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for x,y in zip(fake_x,fake_y): \n",
    "        \n",
    "        pred = f(x) #predict\n",
    "        loss = error(pred,y) #compute loss\n",
    "        loss.backward() # This does backpropagation and sets .grad attribute.\n",
    "\n",
    "        # Update parameters via SGD:\n",
    "        with torch.no_grad(): # This deactivated gradient calculations\n",
    "            \n",
    "            mean_loss += loss.item() # get the raw value of a (1,) tensor\n",
    "            w -= 0.0001 * w.grad # This wouldn't be possible w/ gradient (-= is an inplace operation)\n",
    "            b -= 0.0001 * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Plot the resulting line        \n",
    "    predictions = [f(x) for x in range(100)]\n",
    "    plt.plot(predictions,label=f\"epoch {epoch}\")\n",
    "\n",
    "    print('----')\n",
    "    print(\"loss:\", mean_loss/len(fake_y))\n",
    "    print('w:',w.item())\n",
    "    print('b:',b.item())\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Full pytorch tutorial: \n",
    "\n",
    "A tutorial can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) do not hesitate to take a couple of minutes to skim read it. Plenty of [ressources](https://pytorch.org/resources) are available online. Also, you can have a look at the [extensive pytorch documentation](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Here, as we are defining neural networks, we mainly use the `torch.nn` module which contains most classical deep learning building blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "=> Just like the previous sessions\n",
    "\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:42.999877Z",
     "start_time": "2019-12-13T18:00:42.829015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #train:64535, #val:16133 ,#test:20168\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings.head(5)\n",
    "\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "ratings.head(5)\n",
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following baseline model from surprise\n",
    "\n",
    "## $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) : First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:46.824297Z",
     "start_time": "2019-12-13T18:00:46.768258Z"
    }
   },
   "outputs": [],
   "source": [
    "mu = torch.tensor([train_ratings.rating.mean()],requires_grad=True)#To Complete\n",
    "bu = [torch.tensor([value],requires_grad=True) for value in ratings\\\n",
    "    .groupby('userId')\\\n",
    "    .rating.mean().values.astype(float)]    #--- use a list of torch.tensor() not one torch tensor\n",
    "bi = [torch.tensor([value],requires_grad=True) for value in ratings\\\n",
    "    .groupby('movieId')\\\n",
    "    .rating.mean().values.astype(float)]    #--- use a list of torch.tensor() not one torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:00:53.523954Z",
     "start_time": "2019-12-13T18:00:53.517981Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    \n",
    "    if u < num_users: # if user exist:\n",
    "        user_mean = bu[u]#To Complete\n",
    "    else:\n",
    "        user_mean = torch.tensor([0],requires_grad=True)#To Complete\n",
    "        \n",
    "    if i < num_items: # if item exist:\n",
    "        item_mean = bi[i]#To Complete\n",
    "    else:\n",
    "        item_mean = torch.tensor([0],requires_grad=True)#To Complete\n",
    "        \n",
    "    return mu + user_mean + item_mean#To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:01:00.075267Z",
     "start_time": "2019-12-13T18:01:00.071292Z"
    }
   },
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    return (pred-real).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:01:12.341397Z",
     "start_time": "2019-12-13T18:01:06.839435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train error :  49.73368532512249\n",
      "final val error :  49.709645202552544\n",
      "final test error :  49.73121277886882\n"
     ]
    }
   ],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "    \n",
    "val_e = 0\n",
    "for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    val_e += error(result,r).item()\n",
    "\n",
    "test_e = 0\n",
    "for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    test_e += error(result,r).item()\n",
    "\n",
    "print(\"final train error : \", train_e/len(train_ratings))\n",
    "print(\"final val error : \", val_e/len(val_ratings))\n",
    "print(\"final test error : \", test_e/len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:02:53.580762Z",
     "start_time": "2019-12-13T18:01:18.748400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train error :  0.6999870203798711\n",
      "epoch 0 val error :  0.6655579322341528\n",
      "epoch 0 test error :  0.6440869758603424\n",
      "-----\n",
      "epoch 1 train error :  0.6627290552435594\n",
      "epoch 1 val error :  0.6723392448852467\n",
      "epoch 1 test error :  0.6535317681276341\n",
      "-----\n",
      "epoch 2 train error :  0.6611821344824307\n",
      "epoch 2 val error :  0.6839851419848118\n",
      "epoch 2 test error :  0.6646433754172665\n",
      "-----\n",
      "epoch 3 train error :  0.6593684677792755\n",
      "epoch 3 val error :  0.7576571762281263\n",
      "epoch 3 test error :  0.7349584712091559\n",
      "-----\n",
      "epoch 4 train error :  0.6609458701537142\n",
      "epoch 4 val error :  0.6896433509165538\n",
      "epoch 4 test error :  0.6719101580458979\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_e = 0\n",
    "    \n",
    "    for num,(index, uid, mid, r, ts) in enumerate(train_ratings.sample(frac=1).itertuples()):\n",
    "        result = predict(uid, mid)#To Complete\n",
    "        ex_error = error(result,r)#To Complete\n",
    "        train_e += error(result,r).item()#To Complete\n",
    "        \n",
    "        ex_error.backward()\n",
    "\n",
    "        if num % batch_size == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mu -= lr * mu.grad#To Complete\n",
    "                bu[uid] -= lr * bu[uid].grad#To Complete\n",
    "                bi[mid] -= lr * bi[mid].grad#To Complete\n",
    "\n",
    "                # Manually zero the gradients after updating weights\n",
    "                mu.grad.zero_()\n",
    "                bu[uid].grad.zero_()\n",
    "                bi[mid].grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"epoch {epoch} train error : \", train_e/len(train_ratings))\n",
    "    \n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pytorch (.nn) Modules\n",
    "\n",
    "Instead of having to define everything by hand, pytorch has several usefull abstractions:\n",
    "\n",
    "- `nn.Module()` -> To define the model and the forward computation\n",
    "- `torch.utils.data.DataLoader` -> To create the data pipeline\n",
    "\n",
    "To explore these modules, we'll do the following model:\n",
    "\n",
    "##  Classic SVD (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n",
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:03:02.123734Z",
     "start_time": "2019-12-13T18:03:01.998634Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:03:09.097962Z",
     "start_time": "2019-12-13T18:03:09.086961Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.users = nn.Embedding(nb_users, latent_size)#To Complete     \n",
    "        self.items = nn.Embedding(nb_items, latent_size)#To Complete\n",
    "        #The mean bias\n",
    "        self.mean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out =  self.mean + embed_u @ embed_i.T#To Complete\n",
    "\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:04:00.872665Z",
     "start_time": "2019-12-13T18:03:16.050658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "epoch 0 mse (train/val/test) 16.015 / 15.082 / 14.607\n",
      "-------------------------\n",
      "epoch 1 mse (train/val/test) 14.075 / 15.011 / 14.496\n",
      "-------------------------\n",
      "epoch 2 mse (train/val/test) 13.555 / 15.006 / 14.583\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "\n",
    "\n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train loop\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "        \n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ## Validation loss (no training)\n",
    "        for users_t,items_t,ratings_t in dataloader_val:\n",
    "\n",
    "            model.eval() # Inference mode\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[1] += mse_loss    \n",
    "\n",
    "        ## Test loss (no training)\n",
    "\n",
    "        for users_t,items_t,ratings_t in dataloader_test:\n",
    "            model.eval()\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:13:35.754298Z",
     "start_time": "2019-12-13T18:13:35.742293Z"
    }
   },
   "outputs": [],
   "source": [
    "class KorenMF(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)#To Complete\n",
    "        self.items = nn.Embedding(nb_items, latent_size)#To Complete\n",
    "        self.umean = nn.Embedding(nb_users, 1)#To Complete\n",
    "        self.imean = nn.Embedding(nb_items, 1)#To Complete\n",
    "        self.gmean = nn.Parameter(torch.FloatTensor(1,).fill_(3))#To Complete\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,2,1)\n",
    "        nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = self.gmean + umean + imean + embed_u @ embed_i.T#To Complete\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:17:26.373083Z",
     "start_time": "2019-12-13T18:13:43.540530Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "epoch 0 mse (train/val/test) 81.992 / 35.476 / 34.017\n",
      "-------------------------\n",
      "epoch 1 mse (train/val/test) 29.366 / 26.822 / 26.12\n",
      "-------------------------\n",
      "epoch 2 mse (train/val/test) 23.345 / 23.243 / 22.435\n",
      "-------------------------\n",
      "epoch 3 mse (train/val/test) 19.92 / 21.01 / 20.335\n",
      "-------------------------\n",
      "epoch 4 mse (train/val/test) 17.619 / 19.649 / 19.029\n",
      "-------------------------\n",
      "epoch 5 mse (train/val/test) 16.313 / 18.82 / 18.174\n",
      "-------------------------\n",
      "epoch 6 mse (train/val/test) 15.476 / 18.21 / 17.599\n",
      "-------------------------\n",
      "epoch 7 mse (train/val/test) 14.896 / 17.856 / 17.126\n",
      "-------------------------\n",
      "epoch 8 mse (train/val/test) 14.489 / 17.574 / 16.981\n",
      "-------------------------\n",
      "epoch 9 mse (train/val/test) 14.161 / 17.297 / 16.669\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t,items_t,ratings_t\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction=\"sum\")\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "\n",
    "model =  KorenMF(num_users,num_items,num_feat)#To Complete\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's keras: Pytorch-Lightning\n",
    "\n",
    "Pytorch lightning is a easy to use framework for Pytorch. To start a new project you just need to define two files:\n",
    "\n",
    "- a LightningModule (which inherits `pl.LightningModule`)\n",
    "- a Trainer file. \n",
    "\n",
    "By defining those two files, you get:\n",
    "- Checkpointing\n",
    "- Debugging\n",
    "- Distributed training\n",
    "- Experiment Logging\n",
    "- Training loop\n",
    "- Validation loop\n",
    "- Testing loop\n",
    "\n",
    "## Let's try with the same but different Koren 2009 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "Where the goal is to minimize the following loss\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:17:35.359898Z",
     "start_time": "2019-12-13T18:17:33.335100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningKorenMF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(LightningKorenMF, self).__init__()\n",
    "        \n",
    "        self.reg = 0.001\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)#To Complete\n",
    "        self.items = nn.Embedding(nb_items, latent_size)#To Complete\n",
    "        self.umean = nn.Embedding(nb_users, 1)#To Complete\n",
    "        self.imean = nn.Embedding(nb_items, 1)#To Complete\n",
    "        self.gmean =  nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,0.1,0.1)\n",
    "        nn.init.normal_(self.imean.weight,0.1,0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = self.gmean + umean + imean + embed_u @ embed_i.T#To Complete\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean\n",
    "\n",
    "    \n",
    "    def my_loss_func(self, pred,ratings_t,reg,*params):\n",
    "        '''\n",
    "        mse loss combined with l2 regularization.\n",
    "        params assumed 2-dimension\n",
    "        '''        \n",
    "        mse = F.mse_loss(pred,ratings_t)\n",
    "        l2 = 0\n",
    "        for p in params:\n",
    "            l2 += torch.mean(p.norm(2,-1))\n",
    "\n",
    "        return mse + reg*l2 , mse\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRE\n",
    "        users_t,items_t,ratings_t = batch\n",
    "        pred , *params = self.forward(users_t,items_t) \n",
    "        loss,mse = self.my_loss_func(pred,ratings_t,self.reg,*params)\n",
    "\n",
    "        return {'loss':loss,\"mse\":mse}\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return {\"val_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def validation_end(self,outputs):\n",
    "        return {\"progress_bar\":{\"val_mse\":torch.tensor([output['val_mse'] for output in outputs]).mean().item()}}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return {\"test_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def test_end(self,outputs):\n",
    "        res = {\"progress_bar\":{\"test_mse\":torch.tensor([output['test_mse'] for output in outputs]).mean().item()}}\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)\n",
    "    \n",
    "    def tuple_batch(self,l):\n",
    "        '''\n",
    "        input l: list of (user,item,rating tuples)\n",
    "        output: formatted batches (in torch tensors)\n",
    "\n",
    "        takes n-tuples and create batch\n",
    "        text -> seq word #id\n",
    "        '''\n",
    "        users, items, ratings = zip(*l) \n",
    "        users_t = torch.LongTensor(users)\n",
    "        items_t = torch.LongTensor(items)\n",
    "        ratings_t = torch.FloatTensor(ratings)\n",
    "\n",
    "        return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(prep_train,collate_fn=self.tuple_batch ,num_workers=0, batch_size=32)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_val,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_test,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T18:19:26.217058Z",
     "start_time": "2019-12-13T18:17:42.037947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:    Name       Type Params\n",
      "0  users  Embedding   30 K\n",
      "1  items  Embedding  486 K\n",
      "2  umean  Embedding  610  \n",
      "3  imean  Embedding    9 K\n",
      "Validation sanity check:   0%|                                                                | 0/5 [00:00<?, ?batch/s]C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "Epoch 1:  80%|████████████████████████▊      | 2016/2522 [00:32<00:09, 55.37batch/s, batch_nb=2015, loss=0.966, v_nb=0]C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 23])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "Epoch 1:  80%|████████████████████████▊      | 2017/2522 [00:32<00:09, 55.69batch/s, batch_nb=2016, loss=0.962, v_nb=0]\n",
      "Epoch 1:  84%|██████████████████████████     | 2125/2522 [00:33<00:05, 77.82batch/s, batch_nb=2016, loss=0.962, v_nb=0]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████▌   | 2234/2522 [00:33<00:02, 107.87batch/s, batch_nb=2016, loss=0.962, v_nb=0]\u001b[A\n",
      "Validating:  44%|███████████████████████████▌                                   | 221/505 [00:00<00:00, 1103.34batch/s]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████▍| 2478/2522 [00:33<00:00, 150.84batch/s, batch_nb=2016, loss=0.962, v_nb=0]\u001b[AC:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "Epoch 1: 100%|████████████████| 2522/2522 [00:33<00:00, 150.84batch/s, batch_nb=2016, loss=0.962, v_nb=0, val_mse=0.95]\n",
      "                                                                                                                       \u001b[AC:\\Anaconda3\\envs\\training\\lib\\site-packages\\pytorch_lightning\\callbacks\\pt_callbacks.py:250: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  ' skipping.', RuntimeWarning)\n",
      "Epoch 2:  80%|█████████████▌   | 2017/2522 [00:34<00:09, 50.87batch/s, batch_nb=2016, loss=0.892, v_nb=0, val_mse=0.95]\n",
      "Epoch 2:  82%|█████████████▉   | 2067/2522 [00:34<00:06, 69.86batch/s, batch_nb=2016, loss=0.892, v_nb=0, val_mse=0.95]\u001b[A\n",
      "Epoch 2:  86%|██████████████▋  | 2173/2522 [00:34<00:03, 97.05batch/s, batch_nb=2016, loss=0.892, v_nb=0, val_mse=0.95]\u001b[A\n",
      "Epoch 2:  91%|██████████████▌ | 2298/2522 [00:34<00:01, 134.18batch/s, batch_nb=2016, loss=0.892, v_nb=0, val_mse=0.95]\u001b[A\n",
      "Epoch 2: 100%|███████████████| 2522/2522 [00:34<00:00, 185.91batch/s, batch_nb=2016, loss=0.892, v_nb=0, val_mse=0.951]\u001b[A\n",
      "Epoch 3:  80%|████████████▊   | 2017/2522 [00:35<00:09, 51.35batch/s, batch_nb=2016, loss=0.856, v_nb=0, val_mse=0.951]\u001b[A\n",
      "Epoch 3:  82%|█████████████   | 2061/2522 [00:35<00:06, 70.11batch/s, batch_nb=2016, loss=0.856, v_nb=0, val_mse=0.951]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 2160/2522 [00:35<00:03, 97.21batch/s, batch_nb=2016, loss=0.856, v_nb=0, val_mse=0.951]\u001b[A\n",
      "Epoch 3:  91%|█████████████▋ | 2301/2522 [00:35<00:01, 134.85batch/s, batch_nb=2016, loss=0.856, v_nb=0, val_mse=0.951]\u001b[A\n",
      "Epoch 3: 100%|███████████████| 2522/2522 [00:35<00:00, 187.18batch/s, batch_nb=2016, loss=0.856, v_nb=0, val_mse=0.964]\u001b[A\n",
      "                                                                                                                       \u001b[AC:\\Anaconda3\\envs\\training\\lib\\site-packages\\pytorch_lightning\\callbacks\\pt_callbacks.py:128: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: val_mse\n",
      "  RuntimeWarning)\n",
      "Epoch 3: 100%|████████████████| 2522/2522 [00:35<00:00, 70.94batch/s, batch_nb=2016, loss=0.856, v_nb=0, val_mse=0.964]\n",
      "Testing:  83%|██████████████████████████████████████████████████████▋           | 523/631 [00:00<00:00, 1386.37batch/s]C:\\Anaconda3\\envs\\training\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "Testing: 100%|██████████████████████████████████████████████████████████████████| 631/631 [00:00<00:00, 1600.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'progress_bar': {'test_mse': 0.9300957918167114}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = LightningKorenMF(num_users,num_items,50)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer()    \n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still got time ?\n",
    "\n",
    "[Take a glance at the documentation](https://williamfalcon.github.io/pytorch-lightning/)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
